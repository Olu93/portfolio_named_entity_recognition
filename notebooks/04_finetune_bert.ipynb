{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bfbb9eb-2dd5-4371-8a47-7d9e696b6454",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " # BERT Fine-tuning for Named Entity Recognition (NER)\n",
    "\n",
    " This notebook implements fine-tuning of a pre-trained BERT model for Named Entity Recognition using the CoNLL-2003 format dataset. We'll use the DistilBERT model with custom loss functions and advanced training optimizations to achieve optimal NER performance.\n",
    "\n",
    " ## Setup: Import required libraries and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596db182-6a76-40a5-bd65-fa2d36f297bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ seqeval available for advanced NER metrics\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "from notebook_config import FILES_DIR, DATASETS_DIR\n",
    "import torch\n",
    "from torch.utils.data import Dataset as TorchDataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    AutoConfig\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from transformers import EarlyStoppingCallback\n",
    "from notebooks.notebook_finetune_utils import tokenize_and_align_labels as tokenize_and_align_labels\n",
    "from notebooks.notebook_finetune_utils import DistilBertWithHingeLoss as TokenClassifier\n",
    "# from transformers import AutoModelForTokenClassification as TokenClassifier\n",
    "\n",
    "# Try to import seqeval for better evaluation metrics\n",
    "try:\n",
    "    from seqeval.metrics import precision_score, recall_score, f1_score\n",
    "    SEQEVAL_AVAILABLE = True\n",
    "    print(\"✓ seqeval available for advanced NER metrics\")\n",
    "except ImportError:\n",
    "    print(\"⚠ seqeval not available. Install with: pip install seqeval\")\n",
    "    print(\"   Falling back to simple accuracy metrics\")\n",
    "    SEQEVAL_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d91386e-6c25-452c-9375-fdb3c20d7629",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Define BIO Tag Label Mapping\n",
    "\n",
    " Create the label mapping for BIO (Begin-Inside-Outside) tagging scheme:\n",
    "\n",
    " **BIO Tag Structure:**\n",
    " - **B-**: Beginning of an entity\n",
    " - **I-**: Inside/continuation of an entity\n",
    " - **O**: Outside/not part of any entity\n",
    "\n",
    " **Entity Types:**\n",
    " - **PER**: Person names\n",
    " - **ORG**: Organization names\n",
    " - **LOC**: Location names\n",
    " - **MISC**: Miscellaneous entities\n",
    "\n",
    " This mapping ensures proper conversion between string labels and numeric IDs for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61607ed9-c21d-4588-be62-bf88a4eeb264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping:\n",
      "  O: 0\n",
      "  B-PER: 1\n",
      "  I-PER: 2\n",
      "  B-ORG: 3\n",
      "  I-ORG: 4\n",
      "  B-LOC: 5\n",
      "  I-LOC: 6\n",
      "  B-MISC: 7\n",
      "  I-MISC: 8\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Define the label mapping for BIO tags\n",
    "label2id = {\n",
    "    'O': 0,\n",
    "    'B-PER': 1, 'I-PER': 2,\n",
    "    'B-ORG': 3, 'I-ORG': 4,\n",
    "    'B-LOC': 5, 'I-LOC': 6,\n",
    "    'B-MISC': 7, 'I-MISC': 8\n",
    "}\n",
    "\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "print(\"Label mapping:\")\n",
    "for label, id_ in label2id.items():\n",
    "    print(f\"  {label}: {id_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7508e941-ac5d-478e-aa15-5d85c4819719",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Load and Parse CoNLL Format Data\n",
    "\n",
    " Load the NER annotations from the CoNLL-2003 format file created in the previous notebook:\n",
    "\n",
    " **CoNLL Format Structure:**\n",
    " - Each line contains: `token TAB label`\n",
    " - Empty lines separate sentences\n",
    " - Tokens are pre-tokenized and aligned with their entity labels\n",
    "\n",
    " **Data Validation:**\n",
    " - Checks for valid BIO tags\n",
    " - Handles malformed lines gracefully\n",
    " - Provides detailed error reporting for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dba2ef-9e13-4210-81f2-17a921c4b5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1835 sentences from CoNLL file\n",
      "\n",
      "Example sentences:\n",
      "\n",
      "Sentence 1:\n",
      "  A -> O\n",
      "  drug -> O\n",
      "  is -> O\n",
      "  available -> O\n",
      "  for -> O\n",
      "  monkeypox -> O\n",
      "  patients -> O\n",
      "  who -> O\n",
      "  have -> O\n",
      "  or -> O\n",
      "  ...\n",
      "\n",
      "Sentence 2:\n",
      "  However -> O\n",
      "  , -> O\n",
      "  doctors -> O\n",
      "  across -> O\n",
      "  the -> O\n",
      "  country -> O\n",
      "  suggest -> O\n",
      "  that -> O\n",
      "  significant -> O\n",
      "  barriers -> O\n",
      "  ...\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Load and parse the CoNLL file\n",
    "def load_conll_data(file_path):\n",
    "    \"\"\"Load data from CoNLL format file\"\"\"\n",
    "    sentences = []\n",
    "    current_sentence = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for idx,line in enumerate(f):\n",
    "            line = line.strip()\n",
    "            if line == \"\":\n",
    "                if current_sentence:\n",
    "                    sentences.append(current_sentence)\n",
    "                    current_sentence = []\n",
    "            else:\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 2:\n",
    "                    token = parts[0]\n",
    "                    tag = parts[1]\n",
    "                    if tag not in label2id:\n",
    "                        print(f\"About to do something stupid at line {idx}. The tag is ({tag})\")\n",
    "                    current_sentence.append((token, tag))\n",
    "    \n",
    "    # Add the last sentence if it exists\n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "# Load the CoNLL data\n",
    "conll_file = DATASETS_DIR / \"ner_annotations_combined.conll\"\n",
    "sentences = load_conll_data(conll_file)\n",
    "\n",
    "print(f\"Loaded {len(sentences)} sentences from CoNLL file\")\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\nExample sentences:\")\n",
    "for i, sentence in enumerate(sentences[:2]):\n",
    "    print(f\"\\nSentence {i+1}:\")\n",
    "    for token, tag in sentence[:10]:  # Show first 10 tokens\n",
    "        print(f\"  {token} -> {tag}\")\n",
    "    if len(sentence) > 10:\n",
    "        print(\"  ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6b844d-62aa-4b14-835b-0d3c60a6f17b",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Prepare Data for Tokenization\n",
    "\n",
    " Convert the CoNLL format data into a format suitable for BERT tokenization:\n",
    "\n",
    " **Data Transformation:**\n",
    " - **Token extraction**: Separate tokens from their labels\n",
    " - **Text reconstruction**: Join tokens with spaces for BERT tokenization\n",
    " - **Label preservation**: Maintain corresponding label sequences\n",
    "\n",
    " This step prepares the data for the tokenization process while preserving the entity annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331323c3-f6f6-48c5-be4a-c5049a6f988b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Prepare data for tokenization\n",
    "def prepare_data_for_tokenization(sentences):\n",
    "    \"\"\"Convert sentences to format suitable for tokenization\"\"\"\n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokens = [token for token, _ in sentence]\n",
    "        tags = [tag for _, tag in sentence]\n",
    "        \n",
    "        # Join tokens with spaces for tokenization\n",
    "        text = \" \".join(tokens)\n",
    "        texts.append(text)\n",
    "        labels.append(tags)\n",
    "    \n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a723e6-958a-45c8-8d4a-c104a5a15846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 1835 texts for tokenization\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Prepare the data\n",
    "texts, labels = prepare_data_for_tokenization(sentences)\n",
    "\n",
    "print(f\"Prepared {len(texts)} texts for tokenization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ed198d-d66e-46a5-b24b-b2ac25bcbedb",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Initialize BERT Tokenizer\n",
    "\n",
    " Set up the BERT tokenizer for text processing:\n",
    "\n",
    " **Model Selection:**\n",
    " - **dslim/distilbert-NER**: Pre-trained on NER tasks, optimized for entity recognition\n",
    " - **Cased tokenizer**: Preserves case information important for NER\n",
    " - **Specialized performance**: Better than generic BERT for NER tasks\n",
    "\n",
    " **Tokenizer Features:**\n",
    " - Handles subword tokenization\n",
    " - Manages special tokens (CLS, SEP, PAD)\n",
    " - Provides token-to-word mapping for label alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da199841-18de-4963-910c-c1a84e0cdfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using tokenizer: dslim/distilbert-NER\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Initialize tokenizer - use cased for better NER performance\n",
    "# model_name = \"bert-base-cased\"  # Changed from uncased for better NER\n",
    "model_name = \"dslim/distilbert-NER\"  # Changed from uncased for better NER\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# from notebooks.notebook_finetune_utils import DistilBertWithHingeLoss as TokenClassifier\n",
    "\n",
    "print(f\"Using tokenizer: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c708079-9fd3-4fba-842e-915a9a0a67fe",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Tokenize and Align Labels\n",
    "\n",
    " Apply BERT tokenization to the text data and align the BIO labels with the tokenized output:\n",
    "\n",
    " **Tokenization Process:**\n",
    " - **Subword splitting**: BERT breaks words into subword units\n",
    " - **Label alignment**: Maps original labels to tokenized sequences\n",
    " - **Special token handling**: Properly handles CLS, SEP, and PAD tokens\n",
    " - **Truncation and padding**: Ensures consistent sequence lengths\n",
    "\n",
    " This step is crucial for maintaining the relationship between tokens and their entity labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8838e186-eb30-4ec4-aefe-b203a1bb8143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and aligning labels...\n",
      "Tokenized 1835 sequences\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Tokenize the data\n",
    "print(\"Tokenizing and aligning labels...\")\n",
    "tokenized_inputs, aligned_labels = tokenize_and_align_labels(texts, labels, tokenizer, label2id)\n",
    "\n",
    "print(f\"Tokenized {len(tokenized_inputs['input_ids'])} sequences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f50eabe-09a4-4d9f-8099-d81b62fb4588",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Create PyTorch Dataset Class\n",
    "\n",
    " Define a custom PyTorch Dataset class for efficient data loading during training:\n",
    "\n",
    " **Dataset Features:**\n",
    " - **Efficient indexing**: Fast access to individual samples\n",
    " - **Tensor conversion**: Automatic conversion of labels to PyTorch tensors\n",
    " - **Memory optimization**: Loads data on-demand rather than all at once\n",
    " - **Compatibility**: Works seamlessly with PyTorch DataLoader\n",
    "\n",
    " This class provides the interface between our processed data and the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee79a35-ca90-4223-9455-04c841de9f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Create dataset\n",
    "class NERDataset(TorchDataset):\n",
    "    def __init__(self, tokenized_inputs, labels):\n",
    "        self.tokenized_inputs = tokenized_inputs\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.tokenized_inputs.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7597dfc-ccd2-446f-8139-b9a586ba9c05",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Split Data into Training and Validation Sets\n",
    "\n",
    " Divide the dataset into training and validation subsets for proper model evaluation:\n",
    "\n",
    " **Split Strategy:**\n",
    " - **80% training, 20% validation**: Standard split ratio for NER tasks\n",
    " - **Random state**: Ensures reproducible splits across runs\n",
    " - **Stratified sampling**: Maintains entity distribution across splits\n",
    " - **Independent evaluation**: Prevents data leakage between train and validation\n",
    "\n",
    " This separation is essential for unbiased model performance assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e37083a-79e3-49a9-a4fb-8f5f0717a798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 1468 samples\n",
      "Validation set: 367 samples\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Split data into train and validation\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train set: {len(train_texts)} samples\")\n",
    "print(f\"Validation set: {len(val_labels)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc800c6-40d4-46de-ad37-1e927390f13d",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Tokenize Training and Validation Sets Separately\n",
    "\n",
    " Apply tokenization to the split datasets to prepare them for model training:\n",
    "\n",
    " **Processing Steps:**\n",
    " - **Training set tokenization**: Creates tokenized inputs for model training\n",
    " - **Validation set tokenization**: Prepares data for evaluation during training\n",
    " - **Label alignment**: Ensures proper alignment for both datasets\n",
    " - **Consistent processing**: Same tokenization applied to both splits\n",
    "\n",
    " This step ensures both datasets are properly formatted for the training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49872525-d69c-465e-9d01-72964b239b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing training data...\n",
      "Tokenizing validation data...\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Tokenize train and validation sets\n",
    "print(\"Tokenizing training data...\")\n",
    "train_tokenized, train_aligned_labels = tokenize_and_align_labels(\n",
    "    train_texts, train_labels, tokenizer, label2id\n",
    ")\n",
    "\n",
    "print(\"Tokenizing validation data...\")\n",
    "val_tokenized, val_aligned_labels = tokenize_and_align_labels(\n",
    "    val_texts, val_labels, tokenizer, label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9899c20-e14a-4b91-bcca-64248b883a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 1468\n",
      "Validation dataset size: 367\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = NERDataset(train_tokenized, train_aligned_labels)\n",
    "val_dataset = NERDataset(val_tokenized, val_aligned_labels)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdc405d-6b62-47c5-8279-b273a9a96c73",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Initialize BERT Model with Custom Configuration\n",
    "\n",
    " Set up the BERT model with optimized configuration for NER tasks:\n",
    "\n",
    " **Model Configuration:**\n",
    " - **Pre-trained base**: Uses dslim/distilbert-NER as starting point\n",
    " - **Label count**: Configured for our 9 BIO tag classes\n",
    " - **Dropout settings**: Optimized for regularization and generalization\n",
    " - **Custom loss**: Uses hinge loss for better NER performance\n",
    "\n",
    " **Advanced Features:**\n",
    " - **Attention dropout**: Reduces overfitting in attention mechanisms\n",
    " - **Layer dropout**: Regularizes transformer layers\n",
    " - **Classifier dropout**: Prevents overfitting in the final classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28650f3-4389-4623-8ed7-cfaaaa49d72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized with 9 labels\n",
      "Model config: DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.3,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-PER\",\n",
      "    \"2\": \"I-PER\",\n",
      "    \"3\": \"B-ORG\",\n",
      "    \"4\": \"I-ORG\",\n",
      "    \"5\": \"B-LOC\",\n",
      "    \"6\": \"I-LOC\",\n",
      "    \"7\": \"B-MISC\",\n",
      "    \"8\": \"I-MISC\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"B-LOC\": 5,\n",
      "    \"B-MISC\": 7,\n",
      "    \"B-ORG\": 3,\n",
      "    \"B-PER\": 1,\n",
      "    \"I-LOC\": 6,\n",
      "    \"I-MISC\": 8,\n",
      "    \"I-ORG\": 4,\n",
      "    \"I-PER\": 2,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.53.3\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Initialize model with proper configuration\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    # Dropout settings (DistilBERT)\n",
    "    dropout=0.3,\n",
    "    attention_dropout=0.1,\n",
    "\n",
    "    # Bonus (ignored by DistilBERT but OK to include for compatibility)\n",
    "    hidden_dropout_prob=0.3,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    classifier_dropout=0.3,\n",
    "    summary_first_dropout=0.3,\n",
    "    layerdrop=0.1,\n",
    ")\n",
    "\n",
    "model = TokenClassifier.from_pretrained(\n",
    "    model_name,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(f\"Model initialized with {len(label2id)} labels\")\n",
    "print(f\"Model config: {model.config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ae4729-4054-4cbe-9583-07c5a7358a07",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Set Up Data Collator for Batch Processing\n",
    "\n",
    " Configure the data collator for efficient batch processing during training:\n",
    "\n",
    " **Data Collator Functions:**\n",
    " - **Dynamic padding**: Pads sequences to the longest in each batch\n",
    " - **Label handling**: Properly handles label padding and alignment\n",
    " - **Memory efficiency**: Optimizes memory usage during training\n",
    " - **Batch consistency**: Ensures uniform batch structure\n",
    "\n",
    " This component is essential for efficient training with variable-length sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b260933d-7f7a-46bc-950a-022c0d78e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb70bb2-2c9f-47d6-9635-22900be4099f",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Define Evaluation Metrics Function\n",
    "\n",
    " Create a comprehensive evaluation function that computes NER-specific metrics:\n",
    "\n",
    " **Evaluation Metrics:**\n",
    " - **Precision**: Accuracy of positive predictions\n",
    " - **Recall**: Coverage of actual entities\n",
    " - **F1 Score**: Harmonic mean of precision and recall\n",
    " - **Entity-level evaluation**: Considers complete entity spans\n",
    "\n",
    " **Fallback Handling:**\n",
    " - Uses seqeval for advanced NER metrics when available\n",
    " - Falls back to simple accuracy when seqeval is not installed\n",
    " - Handles special tokens (-100) properly during evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e5d8fb-d1bc-4f14-a98c-675f9212ff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Compute metrics function for evaluation\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"Compute precision, recall, and F1 score for NER\"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(pred, label) if l != -100]\n",
    "        for pred, label in zip(preds, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2label[l] for l in label if l != -100]\n",
    "        for label in labels\n",
    "    ]\n",
    "    \n",
    "    if SEQEVAL_AVAILABLE:\n",
    "        precision = precision_score(true_labels, true_predictions, average=\"weighted\")\n",
    "        recall = recall_score(true_labels, true_predictions, average=\"weighted\")\n",
    "        f1 = f1_score(true_labels, true_predictions, average=\"weighted\")\n",
    "        \n",
    "        return {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "        }\n",
    "    else:\n",
    "        # Fallback to simple accuracy\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for true_pred, true_label in zip(true_predictions, true_labels):\n",
    "            for p, l in zip(true_pred, true_label):\n",
    "                if p == l:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "        \n",
    "        accuracy = correct / total if total > 0 else 0\n",
    "        return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6321ea-9c5e-432c-b06d-d6997f9640ab",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Configure Advanced Training Arguments\n",
    "\n",
    " Set up comprehensive training configuration with optimizations for NER tasks:\n",
    "\n",
    " **Training Optimizations:**\n",
    " - **Learning rate**: 2e-5 for stable fine-tuning\n",
    " - **Batch size**: 5 per device with gradient accumulation for effective batch size of 32\n",
    " - **Mixed precision**: FP16 for faster training and reduced memory usage\n",
    " - **Warmup steps**: 10% of total steps for stable training start\n",
    " - **Early stopping**: Prevents overfitting with patience of 2 epochs\n",
    "\n",
    " **Advanced Features:**\n",
    " - **Gradient accumulation**: Simulates larger batch sizes\n",
    " - **Model checkpointing**: Saves best model based on validation metrics\n",
    " - **Logging**: Comprehensive training progress tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17be90b5-9b69-440a-9dde-3a62002cd8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training arguments:\n",
      "  Total steps: ~273\n",
      "  Warmup steps: 27\n",
      "  Effective batch size: 10\n",
      "  Mixed precision: True\n",
      "  Best metric: precision\n",
      "\n",
      "  Total steps: ~273\n",
      "  Warmup steps: 27\n",
      "  Effective batch size: 10\n",
      "  Mixed precision: True\n",
      "  Best metric: precision\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Training arguments with advanced optimizations\n",
    "total_steps = len(train_dataset) // 16 * 3  # Approximate total steps\n",
    "warmup_steps = int(0.1 * total_steps)  # 10% warmup\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert-ner-model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=5,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=30,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=warmup_steps,  # Warmup for stable training\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,  # More frequent logging\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"precision\" if SEQEVAL_AVAILABLE else \"eval_loss\",\n",
    "    greater_is_better=True if SEQEVAL_AVAILABLE else False,\n",
    "    push_to_hub=False,\n",
    "    fp16=True,  # Mixed precision training for speed and memory\n",
    "    gradient_accumulation_steps=2,  # Effective batch size = 16 * 2 = 32\n",
    "    dataloader_pin_memory=True,  # Faster data loading\n",
    "    remove_unused_columns=False,  # Keep all columns for evaluation\n",
    "    report_to=None,  # Disable wandb/tensorboard reporting\n",
    ")\n",
    "\n",
    "print(f\"Training arguments:\")\n",
    "print(f\"  Total steps: ~{total_steps}\")\n",
    "print(f\"  Warmup steps: {warmup_steps}\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Mixed precision: {training_args.fp16}\")\n",
    "print(f\"  Best metric: {training_args.metric_for_best_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64c91d5-5ad8-4f76-aef8-c0f24dfc69eb",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Initialize Trainer with Callbacks\n",
    "\n",
    " Set up the Hugging Face Trainer with all necessary components:\n",
    "\n",
    " **Trainer Components:**\n",
    " - **Model**: Our configured BERT model\n",
    " - **Datasets**: Training and validation datasets\n",
    " - **Tokenizer**: For text processing during evaluation\n",
    " - **Data collator**: For batch processing\n",
    " - **Metrics function**: For comprehensive evaluation\n",
    " - **Callbacks**: Early stopping to prevent overfitting\n",
    "\n",
    " This trainer provides a complete training pipeline with built-in evaluation and checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998ca480-61b0-4370-8e05-daa12cf292f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-7e00aa91af0c>:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Initialize trainer with metrics\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369caf6d-c817-4a21-99dc-07ce24c2d185",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Execute Model Training\n",
    "\n",
    " Start the fine-tuning process with comprehensive monitoring:\n",
    "\n",
    " **Training Process:**\n",
    " - **Epoch-based training**: 30 epochs with early stopping\n",
    " - **Validation evaluation**: After each epoch\n",
    " - **Progress tracking**: Detailed logging of loss and metrics\n",
    " - **Model checkpointing**: Saves best model based on validation performance\n",
    "\n",
    " **Monitoring Features:**\n",
    " - Real-time loss tracking\n",
    " - Validation metric updates\n",
    " - **Early stopping**: Prevents overfitting\n",
    " - **Best model preservation**: Keeps the best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cec92e4-20b2-4380-877a-9eb3a4e1c0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1176' max='4410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1176/4410 03:30 < 09:39, 5.58 it/s, Epoch 8/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.125500</td>\n",
       "      <td>0.066064</td>\n",
       "      <td>0.311547</td>\n",
       "      <td>0.068839</td>\n",
       "      <td>0.111406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.095900</td>\n",
       "      <td>0.057218</td>\n",
       "      <td>0.330384</td>\n",
       "      <td>0.077818</td>\n",
       "      <td>0.125860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.085600</td>\n",
       "      <td>0.055351</td>\n",
       "      <td>0.323793</td>\n",
       "      <td>0.089458</td>\n",
       "      <td>0.140153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.096200</td>\n",
       "      <td>0.051110</td>\n",
       "      <td>0.397731</td>\n",
       "      <td>0.077486</td>\n",
       "      <td>0.129222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.083000</td>\n",
       "      <td>0.050364</td>\n",
       "      <td>0.435479</td>\n",
       "      <td>0.086797</td>\n",
       "      <td>0.144480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.092900</td>\n",
       "      <td>0.048592</td>\n",
       "      <td>0.461224</td>\n",
       "      <td>0.081144</td>\n",
       "      <td>0.137699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.074500</td>\n",
       "      <td>0.048737</td>\n",
       "      <td>0.427633</td>\n",
       "      <td>0.101763</td>\n",
       "      <td>0.163541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.071200</td>\n",
       "      <td>0.048362</td>\n",
       "      <td>0.422454</td>\n",
       "      <td>0.104090</td>\n",
       "      <td>0.166683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1176, training_loss=0.09910703921804623, metrics={'train_runtime': 211.5285, 'train_samples_per_second': 208.199, 'train_steps_per_second': 20.848, 'total_flos': 1534584212324352.0, 'train_loss': 0.09910703921804623, 'epoch': 8.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "train_results = trainer.train()\n",
    "train_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbdf861-edab-4fae-bb87-c4bf2a12dace",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Evaluate Model Performance\n",
    "\n",
    " Assess the trained model's performance on the validation set:\n",
    "\n",
    " **Evaluation Metrics:**\n",
    " - **Precision**: How many predicted entities are correct\n",
    " - **Recall**: How many actual entities were found\n",
    " - **F1 Score**: Balanced measure of precision and recall\n",
    " - **Loss**: Training and validation loss comparison\n",
    "\n",
    " **Performance Analysis:**\n",
    " - Identifies model strengths and weaknesses\n",
    " - Guides potential model improvements\n",
    " - Validates training effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5901de-5a4f-41a6-9126-137e23edf394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23' max='23' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [23/23 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results:\n",
      "  eval_loss: 0.0486\n",
      "  eval_precision: 0.4612\n",
      "  eval_recall: 0.0811\n",
      "  eval_f1: 0.1377\n",
      "  eval_runtime: 1.9870\n",
      "  eval_samples_per_second: 184.7020\n",
      "  eval_steps_per_second: 11.5750\n",
      "  epoch: 8.0000\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"Evaluating model...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Validation results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25daf173-86bd-4843-92f1-9db15bc596d3",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Test Model on Example Texts\n",
    "\n",
    " Create a prediction function and test the model on various example texts:\n",
    "\n",
    " **Prediction Function Features:**\n",
    " - **Text tokenization**: Proper BERT tokenization with truncation\n",
    " - **Entity prediction**: Token-level entity classification\n",
    " - **Label conversion**: Maps numeric predictions back to BIO tags\n",
    " - **Token alignment**: Handles subword tokenization properly\n",
    "\n",
    " **Test Examples:**\n",
    " - Simple entity examples\n",
    " - Complex medical text\n",
    " - Multi-entity sentences\n",
    " - Edge cases for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd2f095-06ca-4dfa-a313-f11f9afe996f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model on example texts:\n",
      "\n",
      "Text: Joe Biden visited the White House in Washington DC.\n",
      "Predictions:\n",
      "  Joe -> B-PER\n",
      "  B -> I-PER\n",
      "  ##iden -> I-PER\n",
      "  visited -> O\n",
      "  the -> O\n",
      "  White -> B-LOC\n",
      "  House -> I-LOC\n",
      "  in -> O\n",
      "  Washington -> B-LOC\n",
      "  DC -> I-LOC\n",
      "  . -> O\n",
      "\n",
      "Text: Apple CEO Tim Cook announced new products at the conference.\n",
      "Predictions:\n",
      "  Apple -> B-ORG\n",
      "  CEO -> O\n",
      "  Tim -> B-PER\n",
      "  Cook -> I-PER\n",
      "  announced -> O\n",
      "  new -> O\n",
      "  products -> O\n",
      "  at -> O\n",
      "  the -> O\n",
      "  conference -> O\n",
      "  . -> O\n",
      "\n",
      "Text: The United Nations met in New York to discuss climate change.\n",
      "Predictions:\n",
      "  The -> O\n",
      "  United -> B-ORG\n",
      "  Nations -> I-ORG\n",
      "  met -> O\n",
      "  in -> O\n",
      "  New -> B-LOC\n",
      "  York -> I-LOC\n",
      "  to -> O\n",
      "  discuss -> O\n",
      "  climate -> O\n",
      "  change -> O\n",
      "  . -> O\n",
      "\n",
      "Text: A drug is available for monkeypox patients who have or who are at risk of severe disease, but doctors say they continue to face challenges getting access to it. The US Food and Drug Administration hasn't approved tecovirimat – sold under the brand name Tpoxx – specifically for use against monkeypox, but the US Centers for Disease Control and Prevention has made the drug available from the Strategic National Stockpile through expanded access during the global outbreak that has caused about 5,800 probable or confirmed cases in the US.\n",
      "Predictions:\n",
      "  A -> O\n",
      "  drug -> O\n",
      "  is -> O\n",
      "  available -> O\n",
      "  for -> O\n",
      "  monkey -> O\n",
      "  ##pox -> O\n",
      "  patients -> O\n",
      "  who -> O\n",
      "  have -> O\n",
      "  or -> O\n",
      "  who -> O\n",
      "  are -> O\n",
      "  at -> O\n",
      "  risk -> O\n",
      "  of -> O\n",
      "  severe -> O\n",
      "  disease -> O\n",
      "  , -> O\n",
      "  but -> O\n",
      "  doctors -> O\n",
      "  say -> O\n",
      "  they -> O\n",
      "  continue -> O\n",
      "  to -> O\n",
      "  face -> O\n",
      "  challenges -> O\n",
      "  getting -> O\n",
      "  access -> O\n",
      "  to -> O\n",
      "  it -> O\n",
      "  . -> O\n",
      "  The -> O\n",
      "  US -> B-ORG\n",
      "  Food -> I-ORG\n",
      "  and -> I-ORG\n",
      "  Drug -> I-ORG\n",
      "  Administration -> I-ORG\n",
      "  hasn -> O\n",
      "  ' -> O\n",
      "  t -> O\n",
      "  approved -> O\n",
      "  te -> O\n",
      "  ##co -> O\n",
      "  ##vir -> O\n",
      "  ##ima -> O\n",
      "  ##t -> O\n",
      "  – -> O\n",
      "  sold -> O\n",
      "  under -> O\n",
      "  the -> O\n",
      "  brand -> O\n",
      "  name -> O\n",
      "  T -> O\n",
      "  ##pox -> O\n",
      "  ##x -> O\n",
      "  – -> O\n",
      "  specifically -> O\n",
      "  for -> O\n",
      "  use -> O\n",
      "  against -> O\n",
      "  monkey -> O\n",
      "  ##pox -> O\n",
      "  , -> O\n",
      "  but -> O\n",
      "  the -> O\n",
      "  US -> O\n",
      "  Centers -> O\n",
      "  for -> O\n",
      "  Disease -> O\n",
      "  Control -> O\n",
      "  and -> O\n",
      "  Prevention -> O\n",
      "  has -> O\n",
      "  made -> O\n",
      "  the -> O\n",
      "  drug -> O\n",
      "  available -> O\n",
      "  from -> O\n",
      "  the -> O\n",
      "  Strategic -> O\n",
      "  National -> O\n",
      "  Stock -> O\n",
      "  ##pile -> O\n",
      "  through -> O\n",
      "  expanded -> O\n",
      "  access -> O\n",
      "  during -> O\n",
      "  the -> O\n",
      "  global -> O\n",
      "  outbreak -> O\n",
      "  that -> O\n",
      "  has -> O\n",
      "  caused -> O\n",
      "  about -> O\n",
      "  5 -> O\n",
      "  , -> O\n",
      "  800 -> O\n",
      "  probable -> O\n",
      "  or -> O\n",
      "  confirmed -> O\n",
      "  cases -> O\n",
      "  in -> O\n",
      "  the -> O\n",
      "  US -> O\n",
      "  . -> O\n",
      "\n",
      "Text: Tpoxx was FDA-approved in 2018 as the first drug to treat smallpox, a virus in the same family as monkeypox. The World Health Organization declared smallpox eradicated in 1980, but concerns that the virus could be weaponized drove the US government to stockpile more than 1.7 million courses of the drug in case of a bioterrorism event. \n",
      "Predictions:\n",
      "  T -> O\n",
      "  ##pox -> O\n",
      "  ##x -> O\n",
      "  was -> O\n",
      "  FDA -> O\n",
      "  - -> O\n",
      "  approved -> O\n",
      "  in -> O\n",
      "  2018 -> O\n",
      "  as -> O\n",
      "  the -> O\n",
      "  first -> O\n",
      "  drug -> O\n",
      "  to -> O\n",
      "  treat -> O\n",
      "  small -> O\n",
      "  ##pox -> O\n",
      "  , -> O\n",
      "  a -> O\n",
      "  virus -> O\n",
      "  in -> O\n",
      "  the -> O\n",
      "  same -> O\n",
      "  family -> O\n",
      "  as -> O\n",
      "  monkey -> O\n",
      "  ##pox -> O\n",
      "  . -> O\n",
      "  The -> O\n",
      "  World -> B-ORG\n",
      "  Health -> I-ORG\n",
      "  Organization -> I-ORG\n",
      "  declared -> O\n",
      "  small -> O\n",
      "  ##pox -> O\n",
      "  era -> O\n",
      "  ##dicated -> O\n",
      "  in -> O\n",
      "  1980 -> O\n",
      "  , -> O\n",
      "  but -> O\n",
      "  concerns -> O\n",
      "  that -> O\n",
      "  the -> O\n",
      "  virus -> O\n",
      "  could -> O\n",
      "  be -> O\n",
      "  weapon -> O\n",
      "  ##ized -> O\n",
      "  drove -> O\n",
      "  the -> O\n",
      "  US -> O\n",
      "  government -> O\n",
      "  to -> O\n",
      "  stock -> O\n",
      "  ##pile -> O\n",
      "  more -> O\n",
      "  than -> O\n",
      "  1 -> O\n",
      "  . -> O\n",
      "  7 -> O\n",
      "  million -> O\n",
      "  courses -> O\n",
      "  of -> O\n",
      "  the -> O\n",
      "  drug -> O\n",
      "  in -> O\n",
      "  case -> O\n",
      "  of -> O\n",
      "  a -> O\n",
      "  bio -> O\n",
      "  ##ter -> O\n",
      "  ##ro -> O\n",
      "  ##rism -> O\n",
      "  event -> O\n",
      "  . -> O\n",
      "\n",
      "Text: Tpoxx is approved in the European Union to treat monkeypox as well as smallpox. It can be taken intravenously or more commonly as an oral pill. Tpoxx is considered experimental when it comes to monkeypox treatment because there's no data to prove its effectiveness against the disease in humans. Its safety was assessed in healthy humans before its FDA approval for smallpox, and its effectiveness has been tested in animals infected with viruses related to smallpox, including monkeypox. \n",
      "Predictions:\n",
      "  T -> O\n",
      "  ##pox -> O\n",
      "  ##x -> O\n",
      "  is -> O\n",
      "  approved -> O\n",
      "  in -> O\n",
      "  the -> O\n",
      "  European -> B-LOC\n",
      "  Union -> I-LOC\n",
      "  to -> O\n",
      "  treat -> O\n",
      "  monkey -> O\n",
      "  ##pox -> O\n",
      "  as -> O\n",
      "  well -> O\n",
      "  as -> O\n",
      "  small -> O\n",
      "  ##pox -> O\n",
      "  . -> O\n",
      "  It -> O\n",
      "  can -> O\n",
      "  be -> O\n",
      "  taken -> O\n",
      "  in -> O\n",
      "  ##tra -> O\n",
      "  ##ven -> O\n",
      "  ##ously -> O\n",
      "  or -> O\n",
      "  more -> O\n",
      "  commonly -> O\n",
      "  as -> O\n",
      "  an -> O\n",
      "  oral -> O\n",
      "  pill -> O\n",
      "  . -> O\n",
      "  T -> O\n",
      "  ##pox -> O\n",
      "  ##x -> O\n",
      "  is -> O\n",
      "  considered -> O\n",
      "  experimental -> O\n",
      "  when -> O\n",
      "  it -> O\n",
      "  comes -> O\n",
      "  to -> O\n",
      "  monkey -> O\n",
      "  ##pox -> O\n",
      "  treatment -> O\n",
      "  because -> O\n",
      "  there -> O\n",
      "  ' -> O\n",
      "  s -> O\n",
      "  no -> O\n",
      "  data -> O\n",
      "  to -> O\n",
      "  prove -> O\n",
      "  its -> O\n",
      "  effectiveness -> O\n",
      "  against -> O\n",
      "  the -> O\n",
      "  disease -> O\n",
      "  in -> O\n",
      "  humans -> O\n",
      "  . -> O\n",
      "  Its -> O\n",
      "  safety -> O\n",
      "  was -> O\n",
      "  assessed -> O\n",
      "  in -> O\n",
      "  healthy -> O\n",
      "  humans -> O\n",
      "  before -> O\n",
      "  its -> O\n",
      "  FDA -> O\n",
      "  approval -> O\n",
      "  for -> O\n",
      "  small -> O\n",
      "  ##pox -> O\n",
      "  , -> O\n",
      "  and -> O\n",
      "  its -> O\n",
      "  effectiveness -> O\n",
      "  has -> O\n",
      "  been -> O\n",
      "  tested -> O\n",
      "  in -> O\n",
      "  animals -> O\n",
      "  infected -> O\n",
      "  with -> O\n",
      "  viruses -> O\n",
      "  related -> O\n",
      "  to -> O\n",
      "  small -> O\n",
      "  ##pox -> O\n",
      "  , -> O\n",
      "  including -> O\n",
      "  monkey -> O\n",
      "  ##pox -> O\n",
      "  . -> O\n",
      "\n",
      "Text: As the ongoing outbreak increases demand for the drug, the FDA and CDC recently eased some of the administrative requirements that health care providers face when requesting access. However, doctors across the country suggest that significant barriers remain, causing some patients to wait days for shipments or travel to find medical centers that can provide the product at all. \"Patients are trying hard to get this medication, even going out of city or out of state in some cases,\" said Dr. Peter Chin-Hong, an infectious disease physician at UCSF Health.\n",
      "Predictions:\n",
      "  As -> O\n",
      "  the -> O\n",
      "  ongoing -> O\n",
      "  outbreak -> O\n",
      "  increases -> O\n",
      "  demand -> O\n",
      "  for -> O\n",
      "  the -> O\n",
      "  drug -> O\n",
      "  , -> O\n",
      "  the -> O\n",
      "  FDA -> B-ORG\n",
      "  and -> O\n",
      "  CD -> B-ORG\n",
      "  ##C -> I-ORG\n",
      "  recently -> O\n",
      "  eased -> O\n",
      "  some -> O\n",
      "  of -> O\n",
      "  the -> O\n",
      "  administrative -> O\n",
      "  requirements -> O\n",
      "  that -> O\n",
      "  health -> O\n",
      "  care -> O\n",
      "  providers -> O\n",
      "  face -> O\n",
      "  when -> O\n",
      "  requesting -> O\n",
      "  access -> O\n",
      "  . -> O\n",
      "  However -> O\n",
      "  , -> O\n",
      "  doctors -> O\n",
      "  across -> O\n",
      "  the -> O\n",
      "  country -> O\n",
      "  suggest -> O\n",
      "  that -> O\n",
      "  significant -> O\n",
      "  barriers -> O\n",
      "  remain -> O\n",
      "  , -> O\n",
      "  causing -> O\n",
      "  some -> O\n",
      "  patients -> O\n",
      "  to -> O\n",
      "  wait -> O\n",
      "  days -> O\n",
      "  for -> O\n",
      "  shipment -> O\n",
      "  ##s -> O\n",
      "  or -> O\n",
      "  travel -> O\n",
      "  to -> O\n",
      "  find -> O\n",
      "  medical -> O\n",
      "  centers -> O\n",
      "  that -> O\n",
      "  can -> O\n",
      "  provide -> O\n",
      "  the -> O\n",
      "  product -> O\n",
      "  at -> O\n",
      "  all -> O\n",
      "  . -> O\n",
      "  \" -> O\n",
      "  Pat -> O\n",
      "  ##ients -> O\n",
      "  are -> O\n",
      "  trying -> O\n",
      "  hard -> O\n",
      "  to -> O\n",
      "  get -> O\n",
      "  this -> O\n",
      "  medication -> O\n",
      "  , -> O\n",
      "  even -> O\n",
      "  going -> O\n",
      "  out -> O\n",
      "  of -> O\n",
      "  city -> O\n",
      "  or -> O\n",
      "  out -> O\n",
      "  of -> O\n",
      "  state -> O\n",
      "  in -> O\n",
      "  some -> O\n",
      "  cases -> O\n",
      "  , -> O\n",
      "  \" -> O\n",
      "  said -> O\n",
      "  Dr -> O\n",
      "  . -> O\n",
      "  Peter -> O\n",
      "  Chin -> O\n",
      "  - -> O\n",
      "  Hong -> O\n",
      "  , -> O\n",
      "  an -> O\n",
      "  infectious -> O\n",
      "  disease -> O\n",
      "  physician -> O\n",
      "  at -> O\n",
      "  UC -> O\n",
      "  ##SF -> O\n",
      "  Health -> O\n",
      "  . -> O\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Test the model on a few examples\n",
    "def predict_entities(text, model, tokenizer, id2label):\n",
    "    \"\"\"Predict entities in a given text\"\"\"\n",
    "    # Tokenize the text\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=2)\n",
    "    \n",
    "    # Convert predictions to labels\n",
    "    predicted_labels = [id2label[label_id.item()] for label_id in predictions[0]]\n",
    "    \n",
    "    # Align with tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    aligned_predictions = []\n",
    "    \n",
    "    for token, label in zip(tokens, predicted_labels):\n",
    "        if token not in [tokenizer.cls_token, tokenizer.sep_token, tokenizer.pad_token]:\n",
    "            aligned_predictions.append((token, label))\n",
    "    \n",
    "    return aligned_predictions\n",
    "\n",
    "\n",
    "# Test examples\n",
    "test_texts = [\n",
    "    \"Joe Biden visited the White House in Washington DC.\",\n",
    "    \"Apple CEO Tim Cook announced new products at the conference.\",\n",
    "    \"The United Nations met in New York to discuss climate change.\",\n",
    "    \"A drug is available for monkeypox patients who have or who are at risk of severe disease, but doctors say they continue to face challenges getting access to it. The US Food and Drug Administration hasn't approved tecovirimat – sold under the brand name Tpoxx – specifically for use against monkeypox, but the US Centers for Disease Control and Prevention has made the drug available from the Strategic National Stockpile through expanded access during the global outbreak that has caused about 5,800 probable or confirmed cases in the US.\",\n",
    "    \"Tpoxx was FDA-approved in 2018 as the first drug to treat smallpox, a virus in the same family as monkeypox. The World Health Organization declared smallpox eradicated in 1980, but concerns that the virus could be weaponized drove the US government to stockpile more than 1.7 million courses of the drug in case of a bioterrorism event. \",\n",
    "    \"Tpoxx is approved in the European Union to treat monkeypox as well as smallpox. It can be taken intravenously or more commonly as an oral pill. Tpoxx is considered experimental when it comes to monkeypox treatment because there's no data to prove its effectiveness against the disease in humans. Its safety was assessed in healthy humans before its FDA approval for smallpox, and its effectiveness has been tested in animals infected with viruses related to smallpox, including monkeypox. \",\n",
    "    \"As the ongoing outbreak increases demand for the drug, the FDA and CDC recently eased some of the administrative requirements that health care providers face when requesting access. However, doctors across the country suggest that significant barriers remain, causing some patients to wait days for shipments or travel to find medical centers that can provide the product at all. \\\"Patients are trying hard to get this medication, even going out of city or out of state in some cases,\\\" said Dr. Peter Chin-Hong, an infectious disease physician at UCSF Health.\"\n",
    "]\n",
    "\n",
    "print(\"Testing model on example texts:\")\n",
    "for text in test_texts:\n",
    "    print(f\"\\nText: {text}\")\n",
    "    predictions = predict_entities(text, model, tokenizer, id2label)\n",
    "    print(\"Predictions:\")\n",
    "    for token, label in predictions:\n",
    "        # if label != 'O':\n",
    "        print(f\"  {token} -> {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc59c31-09a1-4b6d-b5b6-65015e3b86d7",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Save Trained Model and Tokenizer\n",
    "\n",
    " Persist the trained model and tokenizer for future use:\n",
    "\n",
    " **Model Persistence:**\n",
    " - **Model weights**: Complete trained model parameters\n",
    " - **Tokenizer**: Vocabulary and tokenization rules\n",
    " - **Configuration**: Model architecture and settings\n",
    " - **Metadata**: Training information and results\n",
    "\n",
    " **Storage Benefits:**\n",
    " - Enables model reuse without retraining\n",
    " - Supports deployment in production systems\n",
    " - **Version control**: Tracks model iterations\n",
    " - **Sharing**: Allows model distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4729334-d839-4223-9c79-86538a5e6cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to c:\\Users\\User\\Workspace\\work_practice\\interview\\toptal\\take-home-assignment\\files\\pretrained\\dslim_bert_ner_finetuned\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Save the model and tokenizer properly\n",
    "model_save_path = FILES_DIR / \"pretrained\" / \"dslim_bert_ner_finetuned\"\n",
    "trainer.save_model(str(model_save_path))\n",
    "tokenizer.save_pretrained(str(model_save_path))\n",
    "\n",
    "print(f\"Model and tokenizer saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5959428a-dc9c-47ef-9e76-4b649b580f7e",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Save Comprehensive Training Results\n",
    "\n",
    " Export detailed training results and configuration for analysis and reproducibility:\n",
    "\n",
    " **Results Documentation:**\n",
    " - **Model configuration**: Architecture and hyperparameters\n",
    " - **Training metrics**: Performance statistics and evaluation results\n",
    " - **Dataset information**: Training and validation set sizes\n",
    " - **Training configuration**: Learning rate, batch size, epochs, etc.\n",
    "\n",
    " **Analysis Benefits:**\n",
    " - **Reproducibility**: Complete training setup documentation\n",
    " - **Performance tracking**: Historical model performance\n",
    " - **Hyperparameter analysis**: Impact of different settings\n",
    " - **Model comparison**: Basis for comparing different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c1150e-2ca8-4f43-afa5-9f47cb6bfac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results saved to bert_training_results.json\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Save comprehensive training results\n",
    "results = {\n",
    "    \"model_name\": model_name,\n",
    "    \"num_labels\": len(label2id),\n",
    "    \"label_mapping\": label2id,\n",
    "    \"training_samples\": len(train_dataset),\n",
    "    \"validation_samples\": len(val_dataset),\n",
    "    \"eval_results\": eval_results,  # Save all metrics\n",
    "    \"model_save_path\": str(model_save_path),\n",
    "    \"training_config\": {\n",
    "        \"learning_rate\": training_args.learning_rate,\n",
    "        \"batch_size\": training_args.per_device_train_batch_size,\n",
    "        \"effective_batch_size\": training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps,\n",
    "        \"epochs\": training_args.num_train_epochs,\n",
    "        \"warmup_steps\": training_args.warmup_steps,\n",
    "        \"fp16\": training_args.fp16,\n",
    "    }\n",
    "}\n",
    "\n",
    "model_save_path_results = model_save_path / \"results\"\n",
    "model_save_path_results.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(model_save_path_results / \"bert_training_results.json\", 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Training results saved to bert_training_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85cdce9-9df0-4810-84bc-f8417617fc94",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Training Completion Summary\n",
    "\n",
    " Provide a comprehensive summary of the training process and next steps:\n",
    "\n",
    " **Training Summary:**\n",
    " - **Model location**: Where the trained model is saved\n",
    " - **Results location**: Where training metrics are stored\n",
    " - **Usage instructions**: How to load and use the model\n",
    " - **Performance highlights**: Key achievements and metrics\n",
    "\n",
    " **Next Steps:**\n",
    " - Model deployment instructions\n",
    " - Inference code examples\n",
    " - Potential improvements and optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ad800b-b2e1-4dd9-bb19-cff2233c1554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TRAINING COMPLETED SUCCESSFULLY!\n",
      "==================================================\n",
      "Model saved to: c:\\Users\\User\\Workspace\\work_practice\\interview\\toptal\\take-home-assignment\\files\\pretrained\\dslim_bert_ner_finetuned\n",
      "Results saved to: c:\\Users\\User\\Workspace\\work_practice\\interview\\toptal\\take-home-assignment\\files\\pretrained\\dslim_bert_ner_finetuned\\results\\bert_training_results.json\n",
      "\n",
      "To use the model for inference:\n",
      "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
      "tokenizer = AutoTokenizer.from_pretrained('c:\\Users\\User\\Workspace\\work_practice\\interview\\toptal\\take-home-assignment\\files\\pretrained\\dslim_bert_ner_finetuned')\n",
      "model = AutoModelForTokenClassification.from_pretrained('c:\\Users\\User\\Workspace\\work_practice\\interview\\toptal\\take-home-assignment\\files\\pretrained\\dslim_bert_ner_finetuned')\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Optional: Test with different BERT variants\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Model saved to: {model_save_path}\")\n",
    "print(f\"Results saved to: {model_save_path_results / 'bert_training_results.json'}\")\n",
    "print(\"\\nTo use the model for inference:\")\n",
    "print(f\"from transformers import AutoTokenizer, AutoModelForTokenClassification\")\n",
    "print(f\"tokenizer = AutoTokenizer.from_pretrained('{model_save_path}')\")\n",
    "print(f\"model = AutoModelForTokenClassification.from_pretrained('{model_save_path}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34d07a5-3b39-4911-8b32-7ef28c762551",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Save Training Progress History\n",
    "\n",
    " Export the complete training log history for detailed analysis:\n",
    "\n",
    " **Training History:**\n",
    " - **Step-by-step metrics**: Loss and evaluation metrics at each step\n",
    " - **Learning curves**: Training and validation performance over time\n",
    " - **Convergence analysis**: How the model learned during training\n",
    " - **Debugging information**: Detailed logs for troubleshooting\n",
    "\n",
    " **Analysis Benefits:**\n",
    " - **Learning curve analysis**: Identify training patterns\n",
    " - **Overfitting detection**: Monitor validation vs training performance\n",
    " - **Hyperparameter tuning**: Guide future optimization efforts\n",
    " - **Model comparison**: Compare different training runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4389b5-2362-46f7-b1e0-605ad3c88bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# NOTE 520 Left\n",
    "# save training progress "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bb1efb-a9b3-4a20-a88f-d67866080393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "train_log_history = trainer.state.log_history\n",
    "with open(model_save_path_results / \"bert_training_log_history.json\", 'w') as f:\n",
    "    json.dump(train_log_history, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0202d1c1-b042-4e93-bfa2-f3953f5e23e0",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Final Model Testing\n",
    "\n",
    " Perform final validation tests on the trained model to ensure quality and readiness for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d2b8ae-9fc1-47a1-9584-a4023839f65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# test the model"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
